{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-wx8uA6njZb"
      },
      "source": [
        "# üîç AI Visibility Analyzer\n",
        "\n",
        "**Query Fan-Out + Content Chunking + Similarity Scoring**\n",
        "\n",
        "## üìã Istruzioni:\n",
        "1. Esegui **Cella 1** ‚Üí aspetta restart (10 sec)\n",
        "2. Esegui **Cella 2-8** in ordine\n",
        "3. Configura URL nella **Cella 6**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj8XMNmtnjZd"
      },
      "source": [
        "## üì¶ Cella 1: Installazione\n",
        "\n",
        "‚ö†Ô∏è **Il runtime si riavvier√† - √® normale!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1PKNUfnnjZe",
        "outputId": "02320823-15f6-48d8-81f3-329b5806b2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "jaxlib 0.7.2 requires scipy>=1.13, which is not installed.\n",
            "tensorflow-decision-forests 1.12.0 requires pandas, which is not installed.\n",
            "bokeh 3.7.3 requires pandas>=1.2, which is not installed.\n",
            "scs 3.2.9 requires scipy, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires scipy>=1.3.1, which is not installed.\n",
            "arviz 0.22.0 requires pandas>=2.1.0, which is not installed.\n",
            "arviz 0.22.0 requires scipy>=1.11.0, which is not installed.\n",
            "datasets 4.0.0 requires pandas, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "xarray 2025.10.1 requires pandas>=2.2, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "pandas-gbq 0.29.2 requires pandas>=1.1.4, which is not installed.\n",
            "geemap 0.35.3 requires pandas, which is not installed.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "statsmodels 0.14.5 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "statsmodels 0.14.5 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "panel 1.8.2 requires pandas>=1.2, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "dask-cuda 25.6.0 requires pandas>=1.3, which is not installed.\n",
            "db-dtypes 1.4.3 requires pandas>=1.5.3, which is not installed.\n",
            "bigquery-magics 0.10.3 requires pandas>=1.2.0, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "albumentations 2.0.8 requires scipy>=1.10.0, which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "fastai 2.8.5 requires pandas, which is not installed.\n",
            "fastai 2.8.5 requires scikit-learn, which is not installed.\n",
            "fastai 2.8.5 requires scipy, which is not installed.\n",
            "clarabel 0.11.1 requires scipy, which is not installed.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "lightgbm 4.6.0 requires scipy, which is not installed.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "pytensor 2.35.1 requires scipy<2,>=1, which is not installed.\n",
            "prophet 1.1.7 requires pandas>=1.0.4, which is not installed.\n",
            "bigframes 2.27.0 requires pandas>=1.5.3, which is not installed.\n",
            "yfinance 0.2.66 requires pandas>=1.3.0, which is not installed.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, which is not installed.\n",
            "cuml-cu12 25.6.0 requires scipy>=1.8.0, which is not installed.\n",
            "stumpy 1.13.0 requires scipy>=1.10, which is not installed.\n",
            "xgboost 3.1.1 requires scipy, which is not installed.\n",
            "osqp 1.0.5 requires scipy>=0.13.2, which is not installed.\n",
            "geopandas 1.1.1 requires pandas>=2.0.0, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "cvxpy 1.6.7 requires scipy>=1.11.0, which is not installed.\n",
            "imbalanced-learn 0.14.0 requires scikit-learn<2,>=1.4.2, which is not installed.\n",
            "imbalanced-learn 0.14.0 requires scipy<2,>=1.11.4, which is not installed.\n",
            "tsfresh 0.21.1 requires pandas>=0.25.0, which is not installed.\n",
            "tsfresh 0.21.1 requires scikit-learn>=0.22.0, which is not installed.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "pymc 5.26.1 requires pandas>=0.24.0, which is not installed.\n",
            "pymc 5.26.1 requires scipy>=1.4.1, which is not installed.\n",
            "holoviews 1.21.0 requires pandas>=1.3, which is not installed.\n",
            "gradio 5.49.1 requires pandas<3.0,>=1.0, which is not installed.\n",
            "xarray-einstats 0.9.1 requires scipy>=1.11, which is not installed.\n",
            "libpysal 4.13.0 requires pandas>=1.4, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "shap 0.49.1 requires pandas, which is not installed.\n",
            "shap 0.49.1 requires scikit-learn, which is not installed.\n",
            "shap 0.49.1 requires scipy, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "cmdstanpy 1.3.0 requires pandas, which is not installed.\n",
            "jax 0.7.2 requires scipy>=1.13, which is not installed.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m172.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m168.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m306.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m345.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m345.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "jaxlib 0.7.2 requires scipy>=1.13, which is not installed.\n",
            "scs 3.2.9 requires scipy, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires scipy>=1.3.1, which is not installed.\n",
            "arviz 0.22.0 requires scipy>=1.11.0, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "statsmodels 0.14.5 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "albumentations 2.0.8 requires scipy>=1.10.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "fastai 2.8.5 requires scikit-learn, which is not installed.\n",
            "fastai 2.8.5 requires scipy, which is not installed.\n",
            "clarabel 0.11.1 requires scipy, which is not installed.\n",
            "lightgbm 4.6.0 requires scipy, which is not installed.\n",
            "pytensor 2.35.1 requires scipy<2,>=1, which is not installed.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, which is not installed.\n",
            "cuml-cu12 25.6.0 requires scipy>=1.8.0, which is not installed.\n",
            "stumpy 1.13.0 requires scipy>=1.10, which is not installed.\n",
            "xgboost 3.1.1 requires scipy, which is not installed.\n",
            "osqp 1.0.5 requires scipy>=0.13.2, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "cvxpy 1.6.7 requires scipy>=1.11.0, which is not installed.\n",
            "imbalanced-learn 0.14.0 requires scikit-learn<2,>=1.4.2, which is not installed.\n",
            "imbalanced-learn 0.14.0 requires scipy<2,>=1.11.4, which is not installed.\n",
            "tsfresh 0.21.1 requires scikit-learn>=0.22.0, which is not installed.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "pymc 5.26.1 requires scipy>=1.4.1, which is not installed.\n",
            "xarray-einstats 0.9.1 requires scipy>=1.11, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "shap 0.49.1 requires scikit-learn, which is not installed.\n",
            "shap 0.49.1 requires scipy, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "jax 0.7.2 requires scipy>=1.13, which is not installed.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "!pip uninstall -y numpy pandas scipy scikit-learn -qq 2>/dev/null\n",
        "!pip install numpy==1.26.4 -q\n",
        "!pip install --force-reinstall --no-cache-dir pandas==2.2.2 -q\n",
        "!pip install dspy-ai openai google-generativeai sentence-transformers beautifulsoup4 lxml -q\n",
        "\n",
        "print('‚úÖ Installazione completata')\n",
        "print('üîÑ Riavvio runtime...')\n",
        "\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPSAdj-injZf"
      },
      "source": [
        "## üîß Cella 2: Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVGQPkqknjZg",
        "outputId": "97c4479f-7948-437f-a1cc-c77475eb0600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Import OK | NumPy: 2.3.4 | Pandas: 2.2.2\n"
          ]
        }
      ],
      "source": [
        "import dspy\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import datetime\n",
        "from typing import List, Dict, Optional, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import userdata\n",
        "from IPython.display import HTML, display\n",
        "from google import genai\n",
        "\n",
        "print(f'‚úÖ Import OK | NumPy: {np.__version__} | Pandas: {pd.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPMHA3mQnjZg"
      },
      "source": [
        "## ‚öôÔ∏è Cella 3: Config\n",
        "\n",
        "**IMPORTANTE:** Clicca üîë e aggiungi `OPENAI_API_KEY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChH1GCn9njZg",
        "outputId": "3f1c56a4-a4c3-4cab-aa13-4cef7e383ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini Key OK\n",
            "\n",
            "‚úÖ Config OK\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    print('‚úÖ Gemini Key OK')\n",
        "except:\n",
        "    GEMINI_API_KEY = None\n",
        "    print('‚ùå GEMINI_API_KEY mancante!')\n",
        "\n",
        "MIN_QUERIES_SIMPLE = 10\n",
        "MIN_QUERIES_COMPLEX = 20\n",
        "CHUNK_SIZE = 512\n",
        "CHUNK_OVERLAP = 50\n",
        "SIMILARITY_THRESHOLD = 0.65\n",
        "\n",
        "GEMINI_MODEL = 'gemini-2.0-flash-exp'\n",
        "GEMINI_EMBEDDING_MODEL = 'models/text-embedding-004'\n",
        "\n",
        "ALLOWED_ROUTING_FORMATS = [\n",
        "    'web_article', 'faq_page', 'how_to_steps', 'comparison_table',\n",
        "    'buyers_guide', 'checklist', 'product_spec_sheet', 'glossary/definition',\n",
        "    'pricing_page', 'review_roundup', 'tutorial_video/transcript',\n",
        "    'podcast_transcript', 'code_samples/docs', 'api_reference',\n",
        "    'calculator/tool', 'dataset', 'image_gallery', 'map/local_pack',\n",
        "    'forum/qna', 'pdf_whitepaper', 'case_study', 'press_release',\n",
        "    'interactive_widget'\n",
        "]\n",
        "\n",
        "print('\\n‚úÖ Config OK' if GEMINI_API_KEY else '\\n‚ùå Aggiungi GEMINI_API_KEY!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YiAf5cmnjZg"
      },
      "source": [
        "## üõ†Ô∏è Cella 4: Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSE8vqXRnjZh",
        "outputId": "b22217a0-204f-49d6-f846-cd3e5d6986c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Utils OK\n"
          ]
        }
      ],
      "source": [
        "def extract_content_from_url(url):\n",
        "    try:\n",
        "        r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)\n",
        "        r.raise_for_status()\n",
        "        s = BeautifulSoup(r.content, 'lxml')\n",
        "\n",
        "        for t in s(['script', 'style', 'nav', 'footer', 'aside', 'iframe']):\n",
        "            t.decompose()\n",
        "\n",
        "        title = s.find('title')\n",
        "        title_text = title.get_text(strip=True) if title else 'Untitled'\n",
        "\n",
        "        main = s.find('main') or s.find('article') or s.find('body')\n",
        "        if main:\n",
        "            elems = main.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'td'])\n",
        "            content = ' '.join([e.get_text(strip=True) for e in elems])\n",
        "        else:\n",
        "            content = s.get_text(separator=' ', strip=True)\n",
        "\n",
        "        content = re.sub(r'\\s+', ' ', content).strip()\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'title': title_text,\n",
        "            'content': content,\n",
        "            'word_count': len(content.split()),\n",
        "            'url': url\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {'success': False, 'error': str(e), 'url': url}\n",
        "\n",
        "\n",
        "def chunk_text(text, size=512, overlap=50):\n",
        "    sents = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    curr = []\n",
        "    clen = 0\n",
        "\n",
        "    for s in sents:\n",
        "        words = s.split()\n",
        "        slen = len(words)\n",
        "\n",
        "        if clen + slen > size and curr:\n",
        "            chunks.append(' '.join(curr))\n",
        "            curr = (curr[-overlap:] if len(curr) > overlap else curr) + words\n",
        "            clen = len(curr)\n",
        "        else:\n",
        "            curr.extend(words)\n",
        "            clen += slen\n",
        "\n",
        "    if curr:\n",
        "        chunks.append(' '.join(curr))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "print('‚úÖ Utils OK')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cella 4.5: Hybrid Chunking (WordLift + Semantic)\n",
        "\n",
        "def hybrid_chunk_text(text, analyzer, use_grounding=True, url=None):\n",
        "    \"\"\"\n",
        "    Hybrid approach: try Gemini grounding first, fallback to semantic clustering\n",
        "    \"\"\"\n",
        "\n",
        "    if use_grounding and url:\n",
        "        print('   üåê Attempting Gemini grounding chunks...')\n",
        "        try:\n",
        "            # Use Gemini url_context tool\n",
        "            response = analyzer.client.models.generate_content(\n",
        "                model=analyzer.model,\n",
        "                contents=[f\"Analyze and extract key content sections from: {url}\"],\n",
        "                config={\"tools\": [{\"url_context\": {}}]}\n",
        "            )\n",
        "\n",
        "            grounded_chunks = []\n",
        "            if hasattr(response.candidates[0], 'grounding_metadata'):\n",
        "                chunks_data = response.candidates[0].grounding_metadata.grounding_chunks\n",
        "                for chunk in chunks_data:\n",
        "                    if hasattr(chunk, 'retrieved_context'):\n",
        "                        grounded_chunks.append(chunk.retrieved_context.text.strip())\n",
        "\n",
        "            if grounded_chunks:\n",
        "                print(f'   ‚úÖ Got {len(grounded_chunks)} grounded chunks from Gemini')\n",
        "\n",
        "                # Re-chunk if too large\n",
        "                final_chunks = []\n",
        "                for gc in grounded_chunks:\n",
        "                    if len(gc) > 3000:  # ~600 words\n",
        "                        sub = chunk_text(gc, max_chunk_length=500, overlap_words=20)\n",
        "                        final_chunks.extend(sub)\n",
        "                    else:\n",
        "                        final_chunks.append(gc)\n",
        "\n",
        "                return final_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'   ‚ö†Ô∏è Grounding failed: {e}')\n",
        "\n",
        "    # Fallback: semantic clustering\n",
        "    print('   üîÑ Fallback to semantic clustering...')\n",
        "    return semantic_chunk_text(text, analyzer, target_chunk_words=400)\n",
        "\n",
        "\n",
        "def chunk_text(text, max_chunk_length=500, overlap_words=10):\n",
        "    \"\"\"WordLift's mechanical chunking (for re-chunking large grounded chunks)\"\"\"\n",
        "    if not text: return []\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.replace('\\n', ' '))\n",
        "    if not sentences: return [text] if text else []\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) > max_chunk_length and current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            # Overlap: keep last N words\n",
        "            current_chunk = \" \".join(current_chunk.split()[-overlap_words:])\n",
        "        current_chunk += \" \" + sentence\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return [c for c in chunks if c]"
      ],
      "metadata": {
        "id": "lv0SpIz0WRng"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmc0d_tznjZh"
      },
      "source": [
        "## üß† Cella 5: DSPy + Analyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJFs810PnjZh",
        "outputId": "ba1d719a-ef94-4c41-fb16-452278a91ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GeminiAnalyzer with Semantic Chunking OK\n"
          ]
        }
      ],
      "source": [
        "# Cella 5 - GeminiAnalyzer with Semantic Chunking\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "class QueryFanOutWithFacets(dspy.Signature):\n",
        "    \"\"\"Two-step reasoning: identify facets, then generate queries (WordLift approach)\"\"\"\n",
        "    entity_name = dspy.InputField(desc=\"Main entity/topic\")\n",
        "    current_date = dspy.InputField(desc=\"Current date for time-aware queries\")\n",
        "    num_queries = dspy.InputField(desc=\"Number of queries to generate\")\n",
        "\n",
        "    reasoning_about_facets = dspy.OutputField(\n",
        "        desc=\"\"\"Identify 3-5 key information facets for this entity:\n",
        "        - Definitional/Explanatory (core concepts, what/why)\n",
        "        - Practical/Implementation (how-to, tools, methods, step-by-step)\n",
        "        - Comparative/Analytical (benefits, drawbacks, alternatives, comparison)\n",
        "        - Current/Temporal (recent trends, updates as of current_date)\n",
        "        - Related/Adjacent (sub-topics, related concepts, ecosystem)\n",
        "\n",
        "        Distribution target: 20% basic, 40% technical/implementation, 20% advanced, 20% business\"\"\"\n",
        "    )\n",
        "\n",
        "    synthetic_queries = dspy.OutputField(\n",
        "        desc=\"\"\"Generate exactly {num_queries} SPECIFIC, TECHNICAL search queries.\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "1. Avoid generic queries like \"what is X\" or \"why is X important\"\n",
        "2. Focus on IMPLEMENTATION details, tools, methods, examples\n",
        "3. Include specific scenarios, use cases, or contexts\n",
        "4. Reference specific tools, platforms, or technical aspects\n",
        "\n",
        "Query distribution:\n",
        "- 10% basic definitional (only 2 queries max)\n",
        "- 50% TECHNICAL implementation (tools, methods, code, examples)\n",
        "- 20% advanced optimization (troubleshooting, best practices)\n",
        "- 20% business/comparison (pricing, services, ROI)\n",
        "\n",
        "GOOD examples for \"Content Audit\":\n",
        "- \"How to use Google Analytics 4 to track content performance during audit\"\n",
        "- \"Content audit metrics for e-commerce product pages\"\n",
        "- \"Python script for automated content audit analysis\"\n",
        "- \"Content audit checklist for 10,000+ page websites\"\n",
        "\n",
        "BAD examples (too generic):\n",
        "- \"What is a content audit\"\n",
        "- \"Why content audits are important\"\n",
        "- \"Content audit best practices\"\n",
        "\n",
        "Return ONLY valid JSON array: [\"query1\", \"query2\", ...]\"\"\"\n",
        "    )\n",
        "\n",
        "\n",
        "class GeminiAnalyzer:\n",
        "    def __init__(self, gemini_key):\n",
        "        print('ü§ñ Gemini setup...')\n",
        "        self.client = genai.Client(api_key=gemini_key)\n",
        "        self.model = GEMINI_MODEL\n",
        "\n",
        "        os.environ['GOOGLE_API_KEY'] = gemini_key\n",
        "        try:\n",
        "            dspy_lm = dspy.LM(f'gemini/{GEMINI_MODEL}', api_key=gemini_key, max_tokens=2000, temperature=0.7)\n",
        "            dspy.settings.configure(lm=dspy_lm)\n",
        "            self.query_generator = dspy.ChainOfThought(QueryFanOutWithFacets)\n",
        "            print(f'‚úÖ DSPy configured for facets reasoning')\n",
        "        except Exception as e:\n",
        "            print(f'‚ö†Ô∏è DSPy setup failed: {e}')\n",
        "            self.query_generator = None\n",
        "\n",
        "        print(f'‚úÖ LLM: {self.model}')\n",
        "        print(f'‚úÖ Embeddings: {GEMINI_EMBEDDING_MODEL}')\n",
        "\n",
        "    def _generate_queries(self, entity_name, num_queries):\n",
        "        \"\"\"Generate queries with facets reasoning + ROBUST PARSING\"\"\"\n",
        "        if not self.query_generator:\n",
        "            print('‚ö†Ô∏è DSPy not available')\n",
        "            return [], \"DSPy not configured\"\n",
        "\n",
        "        current_date = datetime.datetime.now().strftime(\"%B %d, %Y\")\n",
        "\n",
        "        try:\n",
        "            result = self.query_generator(\n",
        "                entity_name=entity_name,\n",
        "                current_date=current_date,\n",
        "                num_queries=str(num_queries)\n",
        "            )\n",
        "\n",
        "            reasoning = result.reasoning_about_facets if hasattr(result, 'reasoning_about_facets') else \"N/A\"\n",
        "\n",
        "            # AGGRESSIVE PARSING\n",
        "            raw = result.synthetic_queries.strip()\n",
        "\n",
        "            # Remove all markdown\n",
        "            raw = re.sub(r'```json\\s*', '', raw)\n",
        "            raw = re.sub(r'```\\s*', '', raw)\n",
        "            raw = raw.strip()\n",
        "\n",
        "            queries = []\n",
        "\n",
        "            # Try 1: Direct JSON parse\n",
        "            if raw.startswith('[') and raw.endswith(']'):\n",
        "                try:\n",
        "                    queries = json.loads(raw)\n",
        "                    if isinstance(queries, list) and all(isinstance(q, str) for q in queries):\n",
        "                        print(f'   ‚úÖ Parsed via JSON')\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Try 2: Extract JSON array from text\n",
        "            if not queries:\n",
        "                match = re.search(r'\\[.*?\\]', raw, re.DOTALL)\n",
        "                if match:\n",
        "                    try:\n",
        "                        queries = json.loads(match.group(0))\n",
        "                        if isinstance(queries, list) and all(isinstance(q, str) for q in queries):\n",
        "                            print(f'   ‚úÖ Parsed via regex + JSON')\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            # Try 3: Line-by-line with quote cleaning\n",
        "            if not queries:\n",
        "                lines = raw.split('\\n')\n",
        "                for line in lines:\n",
        "                    line = line.strip()\n",
        "\n",
        "                    # Skip empty, brackets, or markdown\n",
        "                    if not line or line in ['[', ']', '```', '```json']:\n",
        "                        continue\n",
        "\n",
        "                    # Remove numbering: \"1. \" or \"- \" or \"* \"\n",
        "                    line = re.sub(r'^\\d+[\\.\\)]\\s*', '', line)\n",
        "                    line = re.sub(r'^[-*‚Ä¢]\\s*', '', line)\n",
        "\n",
        "                    # Remove trailing comma\n",
        "                    line = line.rstrip(',').strip()\n",
        "\n",
        "                    # Remove quotes if present\n",
        "                    if (line.startswith('\"') and line.endswith('\"')) or \\\n",
        "                       (line.startswith(\"'\") and line.endswith(\"'\")):\n",
        "                        line = line[1:-1]\n",
        "\n",
        "                    if line and len(line) > 5:  # Minimum query length\n",
        "                        queries.append(line)\n",
        "\n",
        "                if queries:\n",
        "                    print(f'   ‚úÖ Parsed via line-by-line ({len(queries)} queries)')\n",
        "\n",
        "            # Cleanup and limit\n",
        "            queries = [q.strip().strip('\"').strip(\"'\") for q in queries if q.strip()]\n",
        "            queries = [q for q in queries if len(q) > 5 and not q.startswith('[') and not q.startswith('{')][:num_queries]\n",
        "\n",
        "            print(f'\\nüß† FACETS REASONING:\\n{reasoning}\\n')\n",
        "\n",
        "            return queries, reasoning\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'‚ö†Ô∏è Query generation error: {e}')\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return [], f\"Error: {e}\"\n",
        "\n",
        "    def _extract_entity(self, title, content):\n",
        "        \"\"\"Extract entity using Gemini\"\"\"\n",
        "        prompt = f\"\"\"Extract the main entity/topic.\n",
        "\n",
        "Title: {title}\n",
        "Content: {content[:500]}\n",
        "\n",
        "Return JSON:\n",
        "{{\"entity_name\": \"main topic\", \"reasoning\": \"why\"}}\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.models.generate_content(\n",
        "                model=self.model,\n",
        "                contents=prompt,\n",
        "                config=types.GenerateContentConfig(temperature=0.3)\n",
        "            )\n",
        "            result = response.text.strip().replace('```json', '').replace('```', '').strip()\n",
        "            return json.loads(result)\n",
        "        except:\n",
        "            return {'entity_name': title, 'reasoning': 'Fallback'}\n",
        "\n",
        "    def _embed(self, texts):\n",
        "        \"\"\"Embeddings with Gemini\"\"\"\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            result = self.client.models.embed_content(\n",
        "                model=GEMINI_EMBEDDING_MODEL,\n",
        "                contents=text\n",
        "            )\n",
        "            embeddings.append(result.embeddings[0].values)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "    def analyze(self, url, original_query=None, mode='AI Mode (complex)', threshold=0.65, use_semantic_chunking=True):\n",
        "        \"\"\"Full analysis with facets-based query generation + semantic chunking\"\"\"\n",
        "        print(f'\\n{\"=\"*70}\\nüîç {url}\\n{\"=\"*70}\\n')\n",
        "\n",
        "        # Step 1: Extract content\n",
        "        print('üìÑ Step 1: Extraction...')\n",
        "        cd = extract_content_from_url(url)\n",
        "        if not cd['success']:\n",
        "            return {'success': False, 'error': cd['error'], 'url': url}\n",
        "        print(f'   ‚úÖ {cd[\"word_count\"]} words')\n",
        "\n",
        "        # Step 2: Extract entity\n",
        "        print('\\nüéØ Step 2: Entity...')\n",
        "        ed = self._extract_entity(cd['title'], cd['content'])\n",
        "        print(f'   ‚úÖ {ed[\"entity_name\"]}')\n",
        "\n",
        "        # Step 3: Generate queries WITH FACETS REASONING\n",
        "        print('\\nüåê Step 3: Query Generation (Facets Approach)...')\n",
        "        min_q = MIN_QUERIES_SIMPLE if 'simple' in mode.lower() else MIN_QUERIES_COMPLEX\n",
        "        queries, reasoning = self._generate_queries(ed[\"entity_name\"], min_q)\n",
        "\n",
        "        if not queries:\n",
        "            print('   ‚ùå No queries generated')\n",
        "            return {'success': False, 'error': 'No queries generated', 'url': url}\n",
        "\n",
        "        print(f'   ‚úÖ {len(queries)} queries generated')\n",
        "        print('\\nüìã GENERATED QUERIES:')\n",
        "        for i, q in enumerate(queries, 1):\n",
        "            print(f'   {i}. {q}')\n",
        "\n",
        "        # Step 4: SEMANTIC CHUNKING (iPullRank approach)\n",
        "        print('\\nüì¶ Step 4: Chunking...')\n",
        "        if use_semantic_chunking:\n",
        "            try:\n",
        "                chunks = semantic_chunk_text(\n",
        "                    cd['content'],\n",
        "                    analyzer=self,\n",
        "                    min_chunk_size=3,\n",
        "                    max_chunk_size=15,\n",
        "                    similarity_threshold=0.75\n",
        "                )\n",
        "                print(f'   ‚úÖ {len(chunks)} semantic chunks created')\n",
        "            except Exception as e:\n",
        "                print(f'   ‚ö†Ô∏è Semantic chunking failed: {e}')\n",
        "                print(f'   üîÑ Falling back to mechanical chunking...')\n",
        "                chunks = chunk_text(cd['content'], CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "                print(f'   ‚úÖ {len(chunks)} mechanical chunks')\n",
        "        else:\n",
        "            chunks = chunk_text(cd['content'], CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "            print(f'   ‚úÖ {len(chunks)} mechanical chunks')\n",
        "\n",
        "        # Step 5: Embeddings\n",
        "        print('\\nüßÆ Step 5: Embeddings...')\n",
        "        chunk_emb = self._embed(chunks)\n",
        "        print(f'   ‚úÖ Chunks encoded')\n",
        "\n",
        "        # Step 6: Similarity scoring\n",
        "        print('\\nüéØ Step 6: Similarity scoring...')\n",
        "        results = []\n",
        "        covered = 0\n",
        "        chunk_usage = {}  # Track which chunks are used\n",
        "\n",
        "        for i, qt in enumerate(queries, 1):\n",
        "            if not qt:\n",
        "                continue\n",
        "\n",
        "            qe = self._embed([qt])[0]\n",
        "            sim = np.dot(chunk_emb, qe) / (np.linalg.norm(chunk_emb, axis=1) * np.linalg.norm(qe))\n",
        "            ms = float(np.max(sim))\n",
        "            bi = int(np.argmax(sim))\n",
        "            cov = ms >= threshold\n",
        "\n",
        "            if cov:\n",
        "                covered += 1\n",
        "\n",
        "            # Track chunk usage\n",
        "            if bi not in chunk_usage:\n",
        "                chunk_usage[bi] = 0\n",
        "            chunk_usage[bi] += 1\n",
        "\n",
        "            results.append({\n",
        "                'query': qt,\n",
        "                'max_similarity': round(ms, 4),\n",
        "                'best_chunk_idx': bi,\n",
        "                'covered': cov\n",
        "            })\n",
        "\n",
        "            print(f'   {\"‚úÖ\" if cov else \"‚ùå\"} {i}. {qt[:40]}... {ms:.3f} [chunk #{bi}]')\n",
        "\n",
        "        total = len(results)\n",
        "        score = (covered / total * 100) if total else 0\n",
        "\n",
        "        print(f'\\n{\"=\"*70}\\nüìä {score:.2f}% ({covered}/{total})\\n{\"=\"*70}\\n')\n",
        "\n",
        "        # Chunk usage analysis\n",
        "        print('üìä CHUNK USAGE ANALYSIS:')\n",
        "        for chunk_idx in sorted(chunk_usage.keys()):\n",
        "            usage_count = chunk_usage[chunk_idx]\n",
        "            percentage = (usage_count / total * 100) if total else 0\n",
        "            print(f'   Chunk #{chunk_idx}: used {usage_count}x ({percentage:.1f}%)')\n",
        "\n",
        "        unused_chunks = set(range(len(chunks))) - set(chunk_usage.keys())\n",
        "        if unused_chunks:\n",
        "            print(f'\\n   ‚ö†Ô∏è Unused chunks: {sorted(unused_chunks)}')\n",
        "            print(f'   ‚Üí Possible irrelevant content or missing query coverage')\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'url': url,\n",
        "            'timestamp': datetime.datetime.now(datetime.UTC).isoformat(),\n",
        "            'entity': ed,\n",
        "            'content': {\n",
        "                'title': cd['title'],\n",
        "                'word_count': cd['word_count'],\n",
        "                'chunks_count': len(chunks),\n",
        "                'chunking_method': 'semantic' if use_semantic_chunking else 'mechanical'\n",
        "            },\n",
        "            'query_fanout': {\n",
        "                'original_query': original_query or f'What is {ed[\"entity_name\"]}?',\n",
        "                'search_mode': mode,\n",
        "                'generated_count': total,\n",
        "                'facets_reasoning': reasoning\n",
        "            },\n",
        "            'ai_visibility_score': round(score, 2),\n",
        "            'covered_queries_count': covered,\n",
        "            'total_queries_count': total,\n",
        "            'similarity_threshold': threshold,\n",
        "            'chunk_usage': chunk_usage,\n",
        "            'unused_chunks': list(unused_chunks) if unused_chunks else [],\n",
        "            'query_details': results,\n",
        "            'models': {\n",
        "                'llm': self.model,\n",
        "                'embeddings': GEMINI_EMBEDDING_MODEL\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "print('‚úÖ GeminiAnalyzer with Semantic Chunking OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5AzGtwRnjZh"
      },
      "source": [
        "## üéØ Cella 6: ESEGUI ANALISI\n",
        "\n",
        "**Configura URL qui:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1A-zPNdnjZi",
        "outputId": "f5f5571f-1d99-4a57-baf0-079d987001a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Gemini setup...\n",
            "‚úÖ DSPy configured for facets reasoning\n",
            "‚úÖ LLM: gemini-2.0-flash-exp\n",
            "‚úÖ Embeddings: models/text-embedding-004\n",
            "\n",
            "======================================================================\n",
            "üîç https://francescoragusa.com/content-audit/\n",
            "======================================================================\n",
            "\n",
            "üìÑ Step 1: Extraction...\n",
            "   ‚úÖ 3065 words\n",
            "\n",
            "üéØ Step 2: Entity...\n",
            "   ‚úÖ Content Audit\n",
            "\n",
            "üåê Step 3: Query Generation (Facets Approach)...\n",
            "   ‚úÖ Parsed via JSON\n",
            "\n",
            "üß† FACETS REASONING:\n",
            "Here are 5 key information facets for Content Audit:\n",
            "\n",
            "*   **Definitional/Explanatory:** What is a content audit, types of content audits, key metrics involved.\n",
            "*   **Practical/Implementation:** How to perform a content audit, tools used (e.g., Google Analytics, Screaming Frog, Ahrefs), step-by-step guides, templates.\n",
            "*   **Comparative/Analytical:** Benefits of content audits, drawbacks, alternatives to content audits, comparing different content audit tools.\n",
            "*   **Current/Temporal:** Recent trends in content auditing, updates to content audit tools as of November 2025, impact of AI on content audits.\n",
            "*   **Related/Adjacent:** Content strategy, SEO audit, website audit, information architecture, user experience (UX) analysis.\n",
            "\n",
            "   ‚úÖ 20 queries generated\n",
            "\n",
            "üìã GENERATED QUERIES:\n",
            "   1. Content audit checklist for identifying outdated content on a SaaS blog\n",
            "   2. How to use Screaming Frog for identifying duplicate content during a website audit\n",
            "   3. Content audit template for e-commerce websites using Google Sheets\n",
            "   4. Python script for automating content audit data extraction from Google Analytics 4 API\n",
            "   5. Content audit metrics for measuring the ROI of blog content\n",
            "   6. Content audit for improving Core Web Vitals: a step-by-step guide\n",
            "   7. How to identify and fix keyword cannibalization issues during a content audit\n",
            "   8. Content audit pricing: comparing agency vs. in-house costs\n",
            "   9. Content audit for mobile-first indexing: best practices and tools\n",
            "   10. How to perform a content audit for a multilingual website\n",
            "   11. Content audit vs SEO audit: key differences and when to use each\n",
            "   12. Content audit for voice search optimization: a practical guide\n",
            "   13. Using AI to automate content gap analysis during a content audit\n",
            "   14. Content audit for identifying content repurposing opportunities\n",
            "   15. Content audit for evaluating content accessibility: WCAG compliance checklist\n",
            "   16. Content audit dashboard using Google Data Studio: a step-by-step guide\n",
            "   17. How to use Ahrefs for identifying low-performing content during a content audit\n",
            "   18. Content audit for improving user engagement on a membership website\n",
            "   19. Content audit ROI calculator: a practical template\n",
            "   20. Content audit trends for 2025: impact of AI and personalization\n",
            "\n",
            "üì¶ Step 4: Chunking...\n",
            "   ‚ö†Ô∏è Semantic chunking failed: semantic_chunk_text() got an unexpected keyword argument 'min_chunk_size'\n",
            "   üîÑ Falling back to mechanical chunking...\n",
            "   ‚úÖ 105 mechanical chunks\n",
            "\n",
            "üßÆ Step 5: Embeddings...\n",
            "   ‚úÖ Chunks encoded\n",
            "\n",
            "üéØ Step 6: Similarity scoring...\n",
            "   ‚úÖ 1. Content audit checklist for identifying ... 0.683 [chunk #0]\n",
            "   ‚ùå 2. How to use Screaming Frog for identifyin... 0.638 [chunk #35]\n",
            "   ‚úÖ 3. Content audit template for e-commerce we... 0.754 [chunk #15]\n",
            "   ‚ùå 4. Python script for automating content aud... 0.576 [chunk #58]\n",
            "   ‚úÖ 5. Content audit metrics for measuring the ... 0.653 [chunk #0]\n",
            "   ‚úÖ 6. Content audit for improving Core Web Vit... 0.691 [chunk #0]\n",
            "   ‚ùå 7. How to identify and fix keyword cannibal... 0.633 [chunk #8]\n",
            "   ‚ùå 8. Content audit pricing: comparing agency ... 0.622 [chunk #0]\n",
            "   ‚ùå 9. Content audit for mobile-first indexing:... 0.642 [chunk #101]\n",
            "   ‚úÖ 10. How to perform a content audit for a mul... 0.660 [chunk #0]\n",
            "   ‚úÖ 11. Content audit vs SEO audit: key differen... 0.716 [chunk #5]\n",
            "   ‚úÖ 12. Content audit for voice search optimizat... 0.677 [chunk #5]\n",
            "   ‚úÖ 13. Using AI to automate content gap analysi... 0.665 [chunk #0]\n",
            "   ‚úÖ 14. Content audit for identifying content re... 0.706 [chunk #0]\n",
            "   ‚úÖ 15. Content audit for evaluating content acc... 0.674 [chunk #0]\n",
            "   ‚úÖ 16. Content audit dashboard using Google Dat... 0.683 [chunk #4]\n",
            "   ‚úÖ 17. How to use Ahrefs for identifying low-pe... 0.670 [chunk #72]\n",
            "   ‚ùå 18. Content audit for improving user engagem... 0.627 [chunk #0]\n",
            "   ‚úÖ 19. Content audit ROI calculator: a practica... 0.732 [chunk #4]\n",
            "   ‚ùå 20. Content audit trends for 2025: impact of... 0.634 [chunk #101]\n",
            "\n",
            "======================================================================\n",
            "üìä 65.00% (13/20)\n",
            "======================================================================\n",
            "\n",
            "üìä CHUNK USAGE ANALYSIS:\n",
            "   Chunk #0: used 9x (45.0%)\n",
            "   Chunk #4: used 2x (10.0%)\n",
            "   Chunk #5: used 2x (10.0%)\n",
            "   Chunk #8: used 1x (5.0%)\n",
            "   Chunk #15: used 1x (5.0%)\n",
            "   Chunk #35: used 1x (5.0%)\n",
            "   Chunk #58: used 1x (5.0%)\n",
            "   Chunk #72: used 1x (5.0%)\n",
            "   Chunk #101: used 2x (10.0%)\n",
            "\n",
            "   ‚ö†Ô∏è Unused chunks: [1, 2, 3, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104]\n",
            "   ‚Üí Possible irrelevant content or missing query coverage\n"
          ]
        }
      ],
      "source": [
        "URL = 'https://francescoragusa.com/content-audit/'\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    analyzer = GeminiAnalyzer(GEMINI_API_KEY)\n",
        "    results = analyzer.analyze(URL)\n",
        "else:\n",
        "    print('‚ùå Aggiungi GEMINI_API_KEY nei Secrets!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j67oM32JnjZi"
      },
      "source": [
        "## üìä Cella 7: Visualizza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cella 7 - Visualizza con Tabella\n",
        "\n",
        "try:\n",
        "    if 'results' not in dir():\n",
        "        print('‚ùå Esegui prima la Cella 6 (ESEGUI ANALISI)')\n",
        "    elif not results.get('success'):\n",
        "        print(f'‚ùå Errore analisi: {results.get(\"error\", \"Unknown\")}')\n",
        "    else:\n",
        "        s = results['ai_visibility_score']\n",
        "        c = '#4CAF50' if s >= 80 else '#FF9800' if s >= 60 else '#F44336'\n",
        "\n",
        "        entity_name = results['entity'].get('entity_name', 'Unknown')\n",
        "\n",
        "        # HTML Report Card\n",
        "        html_content = f'''\n",
        "        <div style=\"font-family:Arial;max-width:800px;margin:20px auto;padding:25px;border:3px solid {c};border-radius:12px\">\n",
        "            <h1 style=\"color:{c}\">üîç AI Visibility Report</h1>\n",
        "            <div style=\"background:linear-gradient(135deg,#667eea,#764ba2);padding:40px;border-radius:10px;text-align:center;color:white;margin:20px 0\">\n",
        "                <div style=\"font-size:72px;font-weight:bold\">{s}%</div>\n",
        "                <p style=\"font-size:22px\">‚úÖ {results[\"covered_queries_count\"]} coperte | ‚ùå {results[\"total_queries_count\"]-results[\"covered_queries_count\"]} gap</p>\n",
        "                <p style=\"font-size:14px;opacity:0.9\">Entity: {entity_name} | {results[\"content\"][\"word_count\"]} words | {results[\"content\"][\"chunks_count\"]} chunks</p>\n",
        "            </div>\n",
        "        </div>\n",
        "        '''\n",
        "\n",
        "        display(HTML(html_content))\n",
        "\n",
        "        # TABELLA DETTAGLIATA\n",
        "        print('\\n' + '='*150)\n",
        "        print('üìä QUERY ANALYSIS TABLE')\n",
        "        print('='*150)\n",
        "\n",
        "        # Crea DataFrame\n",
        "        df = pd.DataFrame(results['query_details'])\n",
        "\n",
        "        # Aggiungi colonna status emoji\n",
        "        df['status'] = df['covered'].apply(lambda x: '‚úÖ' if x else '‚ùå')\n",
        "\n",
        "        # Rinomina colonne per display\n",
        "        df_display = df[[\n",
        "            'status',\n",
        "            'query',\n",
        "            'max_similarity',\n",
        "            'best_chunk_idx',\n",
        "            'covered'\n",
        "        ]].copy()\n",
        "\n",
        "        df_display.columns = ['Status', 'Query Sintetica', 'Similarity', 'Best Chunk', 'Covered']\n",
        "\n",
        "        # Formatta similarity\n",
        "        df_display['Similarity'] = df_display['Similarity'].apply(lambda x: f'{x:.3f}')\n",
        "\n",
        "        # Mostra tabella completa\n",
        "        with pd.option_context('display.max_rows', None,\n",
        "                               'display.max_columns', None,\n",
        "                               'display.width', None,\n",
        "                               'display.max_colwidth', 80):\n",
        "            print(df_display.to_string(index=True))\n",
        "\n",
        "        # CHUNK USAGE ANALYSIS\n",
        "        print('\\n' + '='*150)\n",
        "        print('üì¶ CHUNK USAGE ANALYSIS')\n",
        "        print('='*150)\n",
        "\n",
        "        from collections import Counter\n",
        "        chunk_usage = Counter(df['best_chunk_idx'])\n",
        "\n",
        "        print(f'\\n{\"Chunk\":<8} {\"Usage Count\":<15} {\"Usage %\":<10} {\"Top Query\"}')\n",
        "        print('-' * 150)\n",
        "\n",
        "        for chunk_idx in sorted(chunk_usage.keys()):\n",
        "            count = chunk_usage[chunk_idx]\n",
        "            percentage = (count / len(df) * 100)\n",
        "\n",
        "            # Get top query for this chunk\n",
        "            chunk_queries = df[df['best_chunk_idx'] == chunk_idx].sort_values('max_similarity', ascending=False)\n",
        "            top_query = chunk_queries.iloc[0]['query'][:80] if len(chunk_queries) > 0 else 'N/A'\n",
        "\n",
        "            print(f'#{chunk_idx:<7} {count:<15} {percentage:>6.1f}%     {top_query}')\n",
        "\n",
        "        # Unused chunks\n",
        "        all_chunks = set(range(results['content']['chunks_count']))\n",
        "        used_chunks = set(chunk_usage.keys())\n",
        "        unused = all_chunks - used_chunks\n",
        "\n",
        "        if unused:\n",
        "            print(f'\\n‚ö†Ô∏è  Chunks non utilizzati: {sorted(unused)}')\n",
        "            print(f'   ‚Üí Possibile contenuto irrilevante o gap di coverage')\n",
        "\n",
        "        # TOP COVERED vs TOP GAPS\n",
        "        print('\\n' + '='*150)\n",
        "        print('‚úÖ TOP 5 QUERY COPERTE')\n",
        "        print('='*150)\n",
        "\n",
        "        covered = df[df['covered'] == True].sort_values('max_similarity', ascending=False).head(5)\n",
        "\n",
        "        for idx, row in covered.iterrows():\n",
        "            print(f\"\\n{idx+1}. {row['query']}\")\n",
        "            print(f\"   üìä Similarity: {row['max_similarity']:.3f} | Chunk: #{row['best_chunk_idx']}\")\n",
        "\n",
        "        print('\\n' + '='*150)\n",
        "        print('‚ùå TOP 5 CONTENT GAPS (Opportunit√†)')\n",
        "        print('='*150)\n",
        "\n",
        "        gaps = df[df['covered'] == False].sort_values('max_similarity', ascending=False).head(5)\n",
        "\n",
        "        for idx, row in gaps.iterrows():\n",
        "            print(f\"\\n{idx+1}. {row['query']}\")\n",
        "            print(f\"   üìä Similarity: {row['max_similarity']:.3f} (sotto {results['similarity_threshold']})\")\n",
        "            print(f\"   üìç Chunk match: #{row['best_chunk_idx']} (insufficiente)\")\n",
        "\n",
        "        # SUMMARY\n",
        "        print('\\n' + '='*150)\n",
        "        print('üìä EXECUTIVE SUMMARY')\n",
        "        print('='*150)\n",
        "        print(f'Entity:             {entity_name}')\n",
        "        print(f'URL:                {results[\"url\"]}')\n",
        "        print(f'AI Visibility:      {results[\"ai_visibility_score\"]}%')\n",
        "        print(f'Coverage:           {results[\"covered_queries_count\"]}/{results[\"total_queries_count\"]} queries')\n",
        "        print(f'Threshold:          {results[\"similarity_threshold\"]}')\n",
        "        print(f'Content:            {results[\"content\"][\"word_count\"]} words')\n",
        "        print(f'Chunks:             {results[\"content\"][\"chunks_count\"]} ({results[\"content\"][\"chunking_method\"]})')\n",
        "        print(f'Models:             {results[\"models\"][\"llm\"]} + {results[\"models\"][\"embeddings\"]}')\n",
        "        print(f'Timestamp:          {results[\"timestamp\"]}')\n",
        "\n",
        "        # FACETS REASONING\n",
        "        if 'facets_reasoning' in results['query_fanout']:\n",
        "            print('\\n' + '='*150)\n",
        "            print('üß† FACETS REASONING')\n",
        "            print('='*150)\n",
        "            print(results['query_fanout']['facets_reasoning'])\n",
        "\n",
        "except NameError:\n",
        "    print('‚ùå ERRORE: Esegui prima la Cella 6 (ESEGUI ANALISI)')\n",
        "except Exception as e:\n",
        "    print(f'‚ùå ERRORE visualizzazione: {e}')\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wAPpnwSllYwY",
        "outputId": "138691aa-3cf0-4ca7-b759-b662846a7ab2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style=\"font-family:Arial;max-width:800px;margin:20px auto;padding:25px;border:3px solid #FF9800;border-radius:12px\">\n",
              "            <h1 style=\"color:#FF9800\">üîç AI Visibility Report</h1>\n",
              "            <div style=\"background:linear-gradient(135deg,#667eea,#764ba2);padding:40px;border-radius:10px;text-align:center;color:white;margin:20px 0\">\n",
              "                <div style=\"font-size:72px;font-weight:bold\">65.0%</div>\n",
              "                <p style=\"font-size:22px\">‚úÖ 13 coperte | ‚ùå 7 gap</p>\n",
              "                <p style=\"font-size:14px;opacity:0.9\">Entity: Content Audit | 3065 words | 105 chunks</p>\n",
              "            </div>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================================================================================================\n",
            "üìä QUERY ANALYSIS TABLE\n",
            "======================================================================================================================================================\n",
            "   Status                                                                         Query Sintetica Similarity  Best Chunk  Covered\n",
            "0       ‚úÖ                 Content audit checklist for identifying outdated content on a SaaS blog      0.683           0     True\n",
            "1       ‚ùå      How to use Screaming Frog for identifying duplicate content during a website audit      0.638          35    False\n",
            "2       ‚úÖ                      Content audit template for e-commerce websites using Google Sheets      0.754          15     True\n",
            "3       ‚ùå  Python script for automating content audit data extraction from Google Analytics 4 API      0.576          58    False\n",
            "4       ‚úÖ                             Content audit metrics for measuring the ROI of blog content      0.653           0     True\n",
            "5       ‚úÖ                       Content audit for improving Core Web Vitals: a step-by-step guide      0.691           0     True\n",
            "6       ‚ùå           How to identify and fix keyword cannibalization issues during a content audit      0.633           8    False\n",
            "7       ‚ùå                              Content audit pricing: comparing agency vs. in-house costs      0.622           0    False\n",
            "8       ‚ùå                       Content audit for mobile-first indexing: best practices and tools      0.642         101    False\n",
            "9       ‚úÖ                               How to perform a content audit for a multilingual website      0.660           0     True\n",
            "10      ‚úÖ                        Content audit vs SEO audit: key differences and when to use each      0.717           5     True\n",
            "11      ‚úÖ                          Content audit for voice search optimization: a practical guide      0.677           5     True\n",
            "12      ‚úÖ                        Using AI to automate content gap analysis during a content audit      0.665           0     True\n",
            "13      ‚úÖ                         Content audit for identifying content repurposing opportunities      0.706           0     True\n",
            "14      ‚úÖ           Content audit for evaluating content accessibility: WCAG compliance checklist      0.674           0     True\n",
            "15      ‚úÖ                  Content audit dashboard using Google Data Studio: a step-by-step guide      0.683           4     True\n",
            "16      ‚úÖ         How to use Ahrefs for identifying low-performing content during a content audit      0.670          72     True\n",
            "17      ‚ùå                     Content audit for improving user engagement on a membership website      0.627           0    False\n",
            "18      ‚úÖ                                      Content audit ROI calculator: a practical template      0.732           4     True\n",
            "19      ‚ùå                         Content audit trends for 2025: impact of AI and personalization      0.634         101    False\n",
            "\n",
            "======================================================================================================================================================\n",
            "üì¶ CHUNK USAGE ANALYSIS\n",
            "======================================================================================================================================================\n",
            "\n",
            "Chunk    Usage Count     Usage %    Top Query\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "#0       9                 45.0%     Content audit for identifying content repurposing opportunities\n",
            "#4       2                 10.0%     Content audit ROI calculator: a practical template\n",
            "#5       2                 10.0%     Content audit vs SEO audit: key differences and when to use each\n",
            "#8       1                  5.0%     How to identify and fix keyword cannibalization issues during a content audit\n",
            "#15      1                  5.0%     Content audit template for e-commerce websites using Google Sheets\n",
            "#35      1                  5.0%     How to use Screaming Frog for identifying duplicate content during a website aud\n",
            "#58      1                  5.0%     Python script for automating content audit data extraction from Google Analytics\n",
            "#72      1                  5.0%     How to use Ahrefs for identifying low-performing content during a content audit\n",
            "#101     2                 10.0%     Content audit for mobile-first indexing: best practices and tools\n",
            "\n",
            "‚ö†Ô∏è  Chunks non utilizzati: [1, 2, 3, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104]\n",
            "   ‚Üí Possibile contenuto irrilevante o gap di coverage\n",
            "\n",
            "======================================================================================================================================================\n",
            "‚úÖ TOP 5 QUERY COPERTE\n",
            "======================================================================================================================================================\n",
            "\n",
            "3. Content audit template for e-commerce websites using Google Sheets\n",
            "   üìä Similarity: 0.754 | Chunk: #15\n",
            "\n",
            "19. Content audit ROI calculator: a practical template\n",
            "   üìä Similarity: 0.732 | Chunk: #4\n",
            "\n",
            "11. Content audit vs SEO audit: key differences and when to use each\n",
            "   üìä Similarity: 0.717 | Chunk: #5\n",
            "\n",
            "14. Content audit for identifying content repurposing opportunities\n",
            "   üìä Similarity: 0.706 | Chunk: #0\n",
            "\n",
            "6. Content audit for improving Core Web Vitals: a step-by-step guide\n",
            "   üìä Similarity: 0.691 | Chunk: #0\n",
            "\n",
            "======================================================================================================================================================\n",
            "‚ùå TOP 5 CONTENT GAPS (Opportunit√†)\n",
            "======================================================================================================================================================\n",
            "\n",
            "9. Content audit for mobile-first indexing: best practices and tools\n",
            "   üìä Similarity: 0.642 (sotto 0.65)\n",
            "   üìç Chunk match: #101 (insufficiente)\n",
            "\n",
            "2. How to use Screaming Frog for identifying duplicate content during a website audit\n",
            "   üìä Similarity: 0.638 (sotto 0.65)\n",
            "   üìç Chunk match: #35 (insufficiente)\n",
            "\n",
            "20. Content audit trends for 2025: impact of AI and personalization\n",
            "   üìä Similarity: 0.634 (sotto 0.65)\n",
            "   üìç Chunk match: #101 (insufficiente)\n",
            "\n",
            "7. How to identify and fix keyword cannibalization issues during a content audit\n",
            "   üìä Similarity: 0.633 (sotto 0.65)\n",
            "   üìç Chunk match: #8 (insufficiente)\n",
            "\n",
            "18. Content audit for improving user engagement on a membership website\n",
            "   üìä Similarity: 0.627 (sotto 0.65)\n",
            "   üìç Chunk match: #0 (insufficiente)\n",
            "\n",
            "======================================================================================================================================================\n",
            "üìä EXECUTIVE SUMMARY\n",
            "======================================================================================================================================================\n",
            "Entity:             Content Audit\n",
            "URL:                https://francescoragusa.com/content-audit/\n",
            "AI Visibility:      65.0%\n",
            "Coverage:           13/20 queries\n",
            "Threshold:          0.65\n",
            "Content:            3065 words\n",
            "Chunks:             105 (semantic)\n",
            "Models:             gemini-2.0-flash-exp + models/text-embedding-004\n",
            "Timestamp:          2025-11-06T14:22:03.407924+00:00\n",
            "\n",
            "======================================================================================================================================================\n",
            "üß† FACETS REASONING\n",
            "======================================================================================================================================================\n",
            "Here are 5 key information facets for Content Audit:\n",
            "\n",
            "*   **Definitional/Explanatory:** What is a content audit, types of content audits, key metrics involved.\n",
            "*   **Practical/Implementation:** How to perform a content audit, tools used (e.g., Google Analytics, Screaming Frog, Ahrefs), step-by-step guides, templates.\n",
            "*   **Comparative/Analytical:** Benefits of content audits, drawbacks, alternatives to content audits, comparing different content audit tools.\n",
            "*   **Current/Temporal:** Recent trends in content auditing, updates to content audit tools as of November 2025, impact of AI on content audits.\n",
            "*   **Related/Adjacent:** Content strategy, SEO audit, website audit, information architecture, user experience (UX) analysis.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}