{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro_markdown",
      "metadata": {
        "id": "intro_markdown"
      },
      "source": [
        "## ğŸ” Google AI Search: Simulating Query Fan-Out Visibility\n",
        "\n",
        "This notebook explores how well a given URL's content might be visible to Google's AI search mechanisms, specifically by simulating its \"query fan-out\" process. Read more from our blog: \"[Query Fan-Out AI Search](https://wordlift.io/blog/en/query-fan-out-ai-search/)\"\n",
        "\n",
        "**Core Idea (based on recent studies and insights from patents like US 2024/0289407 A1):**\n",
        "1.  **Entity Identification:** Determine the main entity/topic of a URL.\n",
        "2.  **Synthetic Query Generation (Fan-Out Simulation):** For this entity, generate a set of specific sub-queries that Google's AI might internally use to build a comprehensive understanding. This simulation is further informed by mechanisms detailed in patents such as **WO2024064249A1** (_Systems and methods for prompt-based query generation for diverse retrieval_). The relevance of such patent work to Google's AI Mode query fan-out processes has been highlighted by industry analysis, notably by [Michael King of iPullRank](https://ipullrank.com/how-ai-mode-works).\n",
        "3.  **Content Coverage Assessment:** Check if the original URL's content effectively answers these synthetic sub-queries.\n",
        "\n",
        "This approach, using `DSPy`, aims to move towards more robust AI system building, focusing on task-oriented modules rather than intricate prompt engineering.\n",
        "\n",
        "> **Requirements** â€“ A Google Gemini API key.\n",
        "\n",
        "> **PS:** Itâ€™s early-stage. One thing missing is local context extraction (e.g., if the user has a Knowledge Graph with WordLift, we could check if expected passages are present on the website aside from the audited URL. **[Contact us](https://wordlift.io/book-a-demo/)** to learn more).\n",
        "\n",
        "</br>\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "  <a href=\"https://wordlift.io\">\n",
        "    <img width=130px src=\"https://upload.wikimedia.org/wikipedia/commons/4/48/WordLift-logo-horizontal-2024.png\" />\n",
        "    </a>\n",
        "    </td>\n",
        "    <td>\n",
        "      <i>Concept by WordLift</i>\n",
        "      <br/>\n",
        "      <i>Author: Andrea Volpini & AI Assistant</i>\n",
        "      <br/>\n",
        "      <i>Last updated: June 9th, 2025</i>\n",
        "  </td>\n",
        "</table>\n",
        "</br>\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "install_markdown",
      "metadata": {
        "id": "install_markdown"
      },
      "source": [
        "### 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "install_code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install_code",
        "outputId": "2675ced0-82f3-4847-b9cd-e311ac64f4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installing core libraries ---\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m--- Installation Complete ---\n",
            "\n",
            "--- Key Installed Versions ---\n",
            "dspy-ai                                  3.0.3\n",
            "google-generativeai                      0.8.5\n",
            "----------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Installing core libraries ---\")\n",
        "!pip install -U dspy-ai google-generativeai numpy -qq\n",
        "print(\"--- Installation Complete ---\")\n",
        "\n",
        "# Verify installed versions of key packages\n",
        "print(\"\\n--- Key Installed Versions ---\")\n",
        "!pip list | grep dspy-ai\n",
        "!pip list | grep google-generativeai\n",
        "print(\"----------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup_markdown",
      "metadata": {
        "id": "setup_markdown"
      },
      "source": [
        "### 2. Setup API Keys & DSPy Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "setup_code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup_code",
        "outputId": "a8e938cf-76e7-496a-e0a1-4e4d99fd0512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… GEMINI_API_KEY configured.\n",
            "âœ… Shared `genai.Client` initialized for utilities.\n",
            "âœ… DSPy configured successfully.\n",
            "  - LLM for DSPy Fan-Out: âœ… Gemini (DSPy LM) -> gemini/gemini-2.5-flash-preview-05-20\n",
            "  - Client for Utilities: âœ… Initialized\n",
            "     - URL Insight Model: gemini-2.5-flash-preview-05-20\n",
            "     - Embedding Model: models/text-embedding-004\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Setup API Keys & DSPy Configuration (Revised for Gemini + OpenAI Embeddings)\n",
        "\n",
        "from google.colab import userdata # For Colab secrets\n",
        "import os\n",
        "import openai\n",
        "from google import genai\n",
        "import dspy as dspy # For DSPy\n",
        "import requests # Will be used in utility functions\n",
        "from urllib.parse import urlparse # Will be used in utility functions\n",
        "import re # Will be used in utility functions\n",
        "import numpy as np # Will be used in utility functions\n",
        "import json # Will be used in utility functions\n",
        "import datetime # Will be used in utility functions\n",
        "from IPython.display import display, HTML # Will be used later\n",
        "from google.colab import userdata\n",
        "import datetime # For current_date\n",
        "\n",
        "# ------------  API KEY SETUP  --------------\n",
        "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY') or userdata.get('GEMINI_API_KEY')\n",
        "if not GEMINI_API_KEY:\n",
        "    print(\"âŒ ERROR: GEMINI_API_KEY not found. All Gemini operations will fail.\")\n",
        "else:\n",
        "    # Set for LiteLLM as a fallback, but we will pass the key directly.\n",
        "    os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY\n",
        "    print(\"âœ… GEMINI_API_KEY configured.\")\n",
        "\n",
        "# ------------  SHARED GEMINI CLIENT & MODEL DEFINITIONS FOR UTILITIES  --------------\n",
        "gemini_sdk_client_for_utils = None\n",
        "MODEL_FOR_URL_CONTEXT_TOOL = \"gemini-2.5-flash-preview-05-20\"\n",
        "GEMINI_EMBEDDING_MODEL_FOR_CLIENT = \"models/text-embedding-004\"\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    try:\n",
        "        gemini_sdk_client_for_utils = genai.Client(api_key=GEMINI_API_KEY)\n",
        "        print(f\"âœ… Shared `genai.Client` initialized for utilities.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ERROR: Failed to initialize `genai.Client`: {e}\")\n",
        "\n",
        "# ------------  DSPY CONFIGURATION (for Query Fan-Out)  --------------\n",
        "GEMINI_DSPY_LLM_MODEL = \"gemini/gemini-2.5-flash-preview-05-20\"\n",
        "CONCEPTUAL_GEMINI_EMBEDDING_MODEL_FOR_DSPY = f\"gemini/{GEMINI_EMBEDDING_MODEL_FOR_CLIENT.replace('models/','')}\"\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    try:\n",
        "        # ** THE KEY FIX IS HERE: Explicitly pass api_key to dspy.LM **\n",
        "        llm_for_dspy = dspy.LM(\n",
        "            model=GEMINI_DSPY_LLM_MODEL,\n",
        "            api_key=GEMINI_API_KEY,  # Pass the key directly\n",
        "            max_tokens=1500,\n",
        "            temperature=0.5\n",
        "        )\n",
        "        # Also pass the key to the conceptual RM for completeness\n",
        "        rm_for_dspy = dspy.LM(\n",
        "            model=CONCEPTUAL_GEMINI_EMBEDDING_MODEL_FOR_DSPY,\n",
        "            api_key=GEMINI_API_KEY # Pass the key directly\n",
        "        )\n",
        "\n",
        "        dspy.settings.configure(lm=llm_for_dspy, rm=rm_for_dspy)\n",
        "        print(\"âœ… DSPy configured successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ERROR: Failed to configure DSPy with Gemini models: {e}\")\n",
        "        if 'dspy' in locals() and hasattr(dspy, 'settings'): dspy.settings.configure(lm=None, rm=None)\n",
        "else:\n",
        "    print(\"âš ï¸ DSPy not configured because GEMINI_API_KEY is missing.\")\n",
        "\n",
        "\n",
        "# ------------ FINAL CONFIGURATION CHECK & REPORTING VARIABLES ------------\n",
        "# These variables are set here to be used in the final report in Cell 5\n",
        "if 'dspy' in locals() and hasattr(dspy.settings, 'lm') and dspy.settings.lm and hasattr(dspy.settings.lm, 'model'):\n",
        "    JSON_REPORT_LLM_PROVIDER = \"Gemini (DSPy LM)\"\n",
        "    JSON_REPORT_LLM_MODEL = dspy.settings.lm.model\n",
        "    print(f\"  - LLM for DSPy Fan-Out: âœ… {JSON_REPORT_LLM_PROVIDER} -> {JSON_REPORT_LLM_MODEL}\")\n",
        "else:\n",
        "    JSON_REPORT_LLM_PROVIDER = \"N/A\"\n",
        "    JSON_REPORT_LLM_MODEL = \"N/A\"\n",
        "    print(\"  - LLM for DSPy Fan-Out: âŒ Not configured.\")\n",
        "\n",
        "if 'gemini_sdk_client_for_utils' in globals() and gemini_sdk_client_for_utils:\n",
        "    JSON_REPORT_EMBEDDING_PROVIDER = \"Gemini (Direct Client)\"\n",
        "    JSON_REPORT_EMBEDDING_MODEL = GEMINI_EMBEDDING_MODEL_FOR_CLIENT\n",
        "    print(f\"  - Client for Utilities: âœ… Initialized\")\n",
        "    print(f\"     - URL Insight Model: {MODEL_FOR_URL_CONTEXT_TOOL}\")\n",
        "    print(f\"     - Embedding Model: {JSON_REPORT_EMBEDDING_MODEL}\")\n",
        "else:\n",
        "    JSON_REPORT_EMBEDDING_PROVIDER = \"N/A\"\n",
        "    JSON_REPORT_EMBEDDING_MODEL = \"N/A\"\n",
        "    print(\"  - Client for Utilities: âŒ FAILED to initialize.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "utilities_markdown",
      "metadata": {
        "id": "utilities_markdown"
      },
      "source": [
        "### 3. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "GQFx_cfzMdvb",
      "metadata": {
        "id": "GQFx_cfzMdvb"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Utility Functions (Gemini-Only)\n",
        "\n",
        "def get_url_insights_and_content_via_gemini_client(url_to_process: str) -> dict:\n",
        "    \"\"\"\n",
        "    Uses the shared genai.Client with its url_context tool to get the main entity\n",
        "    and grounded content chunks from a URL.\n",
        "    This function is based on your previously confirmed working logic.\n",
        "    \"\"\"\n",
        "    global gemini_sdk_client_for_utils, MODEL_FOR_URL_CONTEXT_TOOL\n",
        "\n",
        "    if not gemini_sdk_client_for_utils:\n",
        "        error_msg = \"Gemini SDK Client not initialized. Run Cell 2 first.\"\n",
        "        print(f\"âŒ CRITICAL ERROR: {error_msg}\")\n",
        "        return {\"status\": \"failure\", \"entity\": None, \"content_chunks\": [], \"error_message\": error_msg}\n",
        "\n",
        "    print(f\"Processing URL with genai.Client ({MODEL_FOR_URL_CONTEXT_TOOL}): {url_to_process}\")\n",
        "\n",
        "    prompt = f\"Analyze the content of the webpage at: {url_to_process}. Then, identify and state the primary subject or main entity of this page. Respond concisely with only the entity name.\"\n",
        "\n",
        "    try:\n",
        "        response = gemini_sdk_client_for_utils.models.generate_content(\n",
        "            model=MODEL_FOR_URL_CONTEXT_TOOL,\n",
        "            contents=[prompt],\n",
        "            config={\"tools\": [{\"url_context\": {}}]}\n",
        "        )\n",
        "\n",
        "        entity_text = None\n",
        "        retrieved_chunks = []\n",
        "\n",
        "        if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:\n",
        "            entity_text = response.candidates[0].content.parts[0].text.strip()\n",
        "\n",
        "        if hasattr(response.candidates[0], 'grounding_metadata') and response.candidates[0].grounding_metadata.grounding_chunks:\n",
        "            chunks_data = response.candidates[0].grounding_metadata.grounding_chunks\n",
        "            print(f\"âœ… Found {len(chunks_data)} grounded content chunks.\")\n",
        "            for chunk in chunks_data:\n",
        "                if hasattr(chunk, 'retrieved_context') and chunk.retrieved_context and hasattr(chunk.retrieved_context, 'text'):\n",
        "                    retrieved_chunks.append(chunk.retrieved_context.text.strip())\n",
        "                elif hasattr(chunk, 'web') and chunk.web and hasattr(chunk.web, 'title'):\n",
        "                    retrieved_chunks.append(chunk.web.title.strip())\n",
        "\n",
        "        if not retrieved_chunks:\n",
        "            if entity_text: # If we got a text response, use it as a chunk.\n",
        "                print(\"âš ï¸ No specific grounded chunks found. Using model's text response as content.\")\n",
        "                retrieved_chunks = [entity_text]\n",
        "            else: # If we got no text and no chunks, it's a failure.\n",
        "                error_msg = \"Gemini did not return any valid content or grounded chunks from the URL.\"\n",
        "                print(f\"âŒ {error_msg}\")\n",
        "                return {\"status\": \"failure\", \"entity\": None, \"content_chunks\": [], \"error_message\": error_msg}\n",
        "\n",
        "        if not entity_text:\n",
        "            print(\"  -> No direct entity text found. Inferring from chunks...\")\n",
        "            entity_prompt = f\"Based on this text, what is the main entity? Respond with only the entity name.\\n\\nTEXT: {retrieved_chunks[0][:1000]}\"\n",
        "            entity_response = genai.GenerativeModel(\"gemini-2.5-flash\").generate_content(entity_prompt)\n",
        "            if entity_response.text: entity_text = entity_response.text.strip()\n",
        "\n",
        "        if entity_text:\n",
        "             entity_text = re.sub(r\"^(The main entity.*?is\\s*:?\\s*)+\", \"\", entity_text, flags=re.IGNORECASE).strip().split('\\n')[0].strip('.\"\\' ')\n",
        "\n",
        "        parsed_domain = urlparse(url_to_process).netloc.replace(\"www.\",\"\").split('.')[0].title()\n",
        "        if not entity_text or (parsed_domain.lower() in entity_text.lower() and len(entity_text.split()) < 4):\n",
        "            entity_text = parsed_domain\n",
        "\n",
        "        print(f\"âœ… Gemini identified entity: '{entity_text}'\")\n",
        "        return {\"status\": \"success\", \"entity\": entity_text, \"content_chunks\": retrieved_chunks}\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error during Gemini API call for URL insights: {e}\"\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return {\"status\": \"failure\", \"entity\": None, \"content_chunks\": [], \"error_message\": error_msg}\n",
        "\n",
        "\n",
        "def get_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate embedding for text using the shared genai.Client and Gemini embedding model.\n",
        "    This version uses the correct `contents` parameter and has the unsupported 'task_type' removed.\n",
        "    \"\"\"\n",
        "    global gemini_sdk_client_for_utils, GEMINI_EMBEDDING_MODEL_FOR_CLIENT\n",
        "\n",
        "    if not text or not gemini_sdk_client_for_utils:\n",
        "        if not gemini_sdk_client_for_utils:\n",
        "            print(\"âŒ CRITICAL ERROR in get_embedding: Shared genai.Client not initialized.\")\n",
        "        return np.array([])\n",
        "\n",
        "    try:\n",
        "        # ** THE FINAL, CORRECTED API CALL **\n",
        "        # No 'task_type' argument.\n",
        "        result = gemini_sdk_client_for_utils.models.embed_content(\n",
        "            model=GEMINI_EMBEDDING_MODEL_FOR_CLIENT,\n",
        "            contents=[text.strip()] # Correctly using 'contents' with a list\n",
        "        )\n",
        "\n",
        "        # The response for a list of contents is an object with an '.embeddings' attribute.\n",
        "        if hasattr(result, 'embeddings') and result.embeddings:\n",
        "            # result.embeddings is a list of embedding objects. We want the values from the first one.\n",
        "            embedding_object = result.embeddings[0]\n",
        "            if hasattr(embedding_object, 'values'):\n",
        "                 return np.array(embedding_object.values).astype('float32')\n",
        "\n",
        "        # Fallback if the structure is not as expected\n",
        "        print(f\"âš ï¸ Warning: Gemini embedding response format unexpected or empty for '{text[:50]}...'.\")\n",
        "        return np.array([])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error in get_embedding with Gemini for '{text[:50]}...': {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray) or vec1.size == 0 or vec2.size == 0: return 0.0\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0: return 0.0\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "\n",
        "def chunk_text(text, max_chunk_length=500, overlap_words=10):\n",
        "    if not text: return []\n",
        "    sentences = re.split(r'(?<=[.!?])\\\\s+', text.replace('\\n', ' '))\n",
        "    if not sentences: return [text] if text else []\n",
        "    chunks, current_chunk = [], \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) > max_chunk_length and current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = \" \".join(current_chunk.split()[-overlap_words:])\n",
        "        current_chunk += \" \" + sentence\n",
        "    if current_chunk: chunks.append(current_chunk.strip())\n",
        "    return [c for c in chunks if c]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dspy_logic_markdown",
      "metadata": {
        "id": "dspy_logic_markdown"
      },
      "source": [
        "### 4. DSPy Signatures and Modules for Fan-Out Simulation (with ChainOfThought)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "hYOva8iJoLfX",
      "metadata": {
        "id": "hYOva8iJoLfX"
      },
      "outputs": [],
      "source": [
        "# Cell 4: DSPy Signatures and Modules for Fan-Out Simulation\n",
        "\n",
        "class GenerateSyntheticQueriesCoT(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Given an entity (typically the main topic of a webpage), the current date, and a desired number of sub-queries,\n",
        "    first, briefly reason about the key facets, common questions, and closely related concepts\n",
        "    a user exploring this entity with an AI search companion (like Google AI Mode) might be interested in.\n",
        "    Then, generate a list of diverse, specific, and factual sub-queries that reflect this exploration.\n",
        "\n",
        "    These sub-queries simulate how an AI might \"fan-out\" its internal searches.\n",
        "    Consider the current date: {current_date} to ensure queries aim for up-to-date information if relevant to the entity.\n",
        "\n",
        "    For the reasoning, identify 3-5 primary facets or types of information related to the entity.\n",
        "    For the synthetic queries, aim for a mix that explores these facets. For example, consider:\n",
        "    - Definitional/Explanatory queries about the entity or its core components.\n",
        "    - Practical application or \"how-to\" queries related to the entity.\n",
        "    - Queries exploring benefits, drawbacks, or comparisons if applicable.\n",
        "    - Queries about recent developments or current status, especially if the entity is time-sensitive, informed by the current date.\n",
        "    - Queries touching on closely related sub-topics or entities.\n",
        "\n",
        "    The final output for 'synthetic_queries' MUST be a Python-parseable list of strings.\n",
        "    Example for entity \"Agile Project Management\", num_queries \"4\", and current_date \"{current_date}\":\n",
        "    Reasoning About Facets: Key facets for Agile Project Management include its core principles, benefits particularly in software development, common tools used by teams, and recent trends or discussions around its adoption as of {current_date}.\n",
        "    Synthetic Queries: [\"what are the core principles of agile project management\", \"how does agile project management benefit software development teams\", \"popular agile project management tools in {current_year_from_date}\", \"latest trends in agile methodology adoption as of {current_date}\"]\n",
        "    \"\"\"\n",
        "    entity_name = dspy.InputField(desc=\"The main entity or topic of a webpage.\")\n",
        "    current_date = dspy.InputField(desc=\"The current date, e.g., 'May 27, 2025'.\")\n",
        "    num_queries_to_generate = dspy.InputField(desc=\"The exact number of distinct sub-queries to generate (e.g., '5').\")\n",
        "\n",
        "    reasoning_about_facets = dspy.OutputField(desc=\"Briefly outline the key information facets or types of user questions considered for the entity. Explain their relevance for a comprehensive understanding, considering the current date if applicable.\")\n",
        "    synthetic_queries = dspy.OutputField(desc=\"A Python list of strings, containing exactly the number of sub-queries specified. Format: [\\\"query1\\\", \\\"query2\\\", ...]\")\n",
        "\n",
        "class GoogleAIQueryFanOut(dspy.Module):\n",
        "    def __init__(self, num_queries_default=5):\n",
        "        super().__init__()\n",
        "        self.num_queries_default = num_queries_default\n",
        "        self.generate_queries_with_cot = dspy.ChainOfThought(GenerateSyntheticQueriesCoT)\n",
        "\n",
        "    def forward(self, entity_name, current_date_str, num_queries=None):\n",
        "        num_to_gen_val = num_queries if num_queries is not None else self.num_queries_default\n",
        "\n",
        "        prediction = self.generate_queries_with_cot(\n",
        "            entity_name=entity_name,\n",
        "            current_date=current_date_str,\n",
        "            num_queries_to_generate=str(num_to_gen_val)\n",
        "        )\n",
        "\n",
        "        reasoning_output = \"No reasoning provided.\"\n",
        "        if hasattr(prediction, 'reasoning_about_facets'):\n",
        "            reasoning_output = prediction.reasoning_about_facets\n",
        "            print(f\"ğŸ§  Reasoning for '{entity_name}' (Date: {current_date_str}, Num queries: {num_to_gen_val}):\\n{reasoning_output}\")\n",
        "\n",
        "        raw_output = prediction.synthetic_queries.strip()\n",
        "        queries = []\n",
        "\n",
        "        if raw_output.startswith('[') and raw_output.endswith(']'):\n",
        "            try:\n",
        "                parsed_list = json.loads(raw_output)\n",
        "                if isinstance(parsed_list, list) and all(isinstance(item, str) for item in parsed_list):\n",
        "                    queries = parsed_list\n",
        "                    # print(f\"Successfully parsed queries using json.loads for '{entity_name}'.\")\n",
        "            except json.JSONDecodeError:\n",
        "                # print(f\"json.loads failed for raw_output: '{raw_output}'. Trying other parsing methods.\")\n",
        "                pass\n",
        "\n",
        "        if not queries:\n",
        "            list_match = re.search(r\"\\[\\s*('([^']*)'|\\\"([^\\\"]*)\\\")\\s*(,\\s*('([^']*)'|\\\"([^\\\"]*)\\\"))*\\s*\\]\", raw_output)\n",
        "            if list_match:\n",
        "                list_str_content = list_match.group(0)\n",
        "                try:\n",
        "                    import ast\n",
        "                    evaluated_list = ast.literal_eval(list_str_content)\n",
        "                    if isinstance(evaluated_list, list) and all(isinstance(item, str) for item in evaluated_list):\n",
        "                        queries = evaluated_list\n",
        "                        # print(f\"Successfully parsed queries using ast.literal_eval for '{entity_name}'.\")\n",
        "                except (ValueError, SyntaxError, TypeError):\n",
        "                    # print(f\"ast.literal_eval failed for list_str_content: '{list_str_content}'. Trying newline split.\")\n",
        "                    pass\n",
        "\n",
        "            if not queries:\n",
        "                potential_queries = raw_output.splitlines()\n",
        "                cleaned_queries = []\n",
        "                for q_line in potential_queries:\n",
        "                    q_line = q_line.strip()\n",
        "                    q_line = re.sub(r\"^\\s*(\\d+\\.|-|\\*)\\s*\", \"\", q_line)\n",
        "                    if (q_line.startswith('\"') and q_line.endswith('\"')) or \\\n",
        "                       (q_line.startswith(\"'\") and q_line.endswith(\"'\")):\n",
        "                        q_line = q_line[1:-1]\n",
        "                    if q_line:\n",
        "                        cleaned_queries.append(q_line)\n",
        "                if cleaned_queries:\n",
        "                    queries = cleaned_queries\n",
        "                    # print(f\"Parsed queries using newline splitting for '{entity_name}'.\")\n",
        "\n",
        "        queries = [str(q).strip() for q in queries if str(q).strip()][:num_to_gen_val]\n",
        "        if not queries and raw_output:\n",
        "            queries = [raw_output]\n",
        "            print(f\"Warning: All parsing failed for '{entity_name}'. Treating raw output as a single query: '{raw_output[:100]}...'\")\n",
        "\n",
        "        return dspy.Prediction(\n",
        "            synthetic_queries=queries,\n",
        "            reasoning_about_facets=reasoning_output\n",
        "        )\n",
        "\n",
        "class ContentCoverageScorer(dspy.Module):\n",
        "    def __init__(self, similarity_threshold_default=0.75):\n",
        "        super().__init__()\n",
        "        self.similarity_threshold_default = similarity_threshold_default\n",
        "\n",
        "    def forward(self, query, content_chunks, similarity_threshold=None):\n",
        "        threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold_default\n",
        "        if not query or not content_chunks:\n",
        "            return dspy.Prediction(is_covered=False, best_matching_chunk=\"\", max_similarity=0.0)\n",
        "\n",
        "        query_embedding = get_embedding(str(query))\n",
        "        if query_embedding.size == 0:\n",
        "            return dspy.Prediction(is_covered=False, best_matching_chunk=\"\", max_similarity=0.0)\n",
        "\n",
        "        max_similarity = 0.0\n",
        "        best_chunk_for_query = \"\"\n",
        "        covered_by_threshold = False\n",
        "\n",
        "        for chunk_text_item in content_chunks:\n",
        "            chunk_embedding = get_embedding(str(chunk_text_item))\n",
        "            if chunk_embedding.size > 0:\n",
        "                sim = cosine_similarity(query_embedding, chunk_embedding)\n",
        "                if sim > max_similarity:\n",
        "                    max_similarity = sim\n",
        "                    best_chunk_for_query = str(chunk_text_item)\n",
        "                if sim >= threshold:\n",
        "                    covered_by_threshold = True\n",
        "\n",
        "        return dspy.Prediction(is_covered=covered_by_threshold, best_matching_chunk=best_chunk_for_query if covered_by_threshold else \"\", max_similarity=float(max_similarity))\n",
        "\n",
        "class AIVisibilityAuditor(dspy.Module):\n",
        "    def __init__(self, num_synthetic_queries_default=5, coverage_threshold_default=0.75):\n",
        "        super().__init__()\n",
        "        self.query_fanout = GoogleAIQueryFanOut(num_queries_default=num_synthetic_queries_default)\n",
        "        self.coverage_scorer = ContentCoverageScorer(similarity_threshold_default=coverage_threshold_default)\n",
        "\n",
        "    def forward(self, url, num_synthetic_queries=None, coverage_threshold=None):\n",
        "        print(f\"\\nğŸš€ Starting AI Visibility Audit for: {url}\")\n",
        "\n",
        "        url_insights = get_url_insights_and_content_via_gemini_client(url)\n",
        "\n",
        "        if url_insights.get(\"status\") == \"failure\":\n",
        "            error_msg = url_insights.get(\"error_message\", \"Unknown Gemini insight error\")\n",
        "            print(f\"âŒ Failed to get insights from Gemini for {url}: {error_msg}. Aborting audit.\")\n",
        "            return dspy.Prediction(\n",
        "                entity_name=\"Gemini Insight Error\",\n",
        "                url=url,\n",
        "                coverage_score=0.0,\n",
        "                audit_details=[],\n",
        "                reasoning_about_facets=f\"Gemini Insight Error: {error_msg}\"\n",
        "            )\n",
        "\n",
        "        main_entity = url_insights.get(\"entity\")\n",
        "        initial_content_chunks = url_insights.get(\"content_chunks\", [])\n",
        "\n",
        "        if not main_entity:\n",
        "            print(f\"âŒ Critical: No main entity could be determined by Gemini for {url}. Cannot generate synthetic queries.\")\n",
        "            return dspy.Prediction(entity_name=\"Entity Extraction Failed (Gemini)\", url=url, coverage_score=0.0, audit_details=[], reasoning_about_facets=\"Entity Extraction Failed (Gemini)\")\n",
        "\n",
        "        print(f\"ğŸ§  Main entity identified by Gemini: '{main_entity}'\")\n",
        "\n",
        "        TOKEN_LIMIT_PROXY_CHARS = 6000\n",
        "        final_content_chunks = []\n",
        "\n",
        "        if initial_content_chunks:\n",
        "            print(f\"ğŸ“„ Obtained {len(initial_content_chunks)} grounded content chunk(s) from Gemini. Validating for embedding limits...\")\n",
        "            for chunk_item in initial_content_chunks:\n",
        "                if len(str(chunk_item)) > TOKEN_LIMIT_PROXY_CHARS:\n",
        "                    print(f\"  -> Chunk is too long ({len(str(chunk_item))} chars). Re-chunking it...\")\n",
        "                    sub_chunks = chunk_text(str(chunk_item), max_chunk_length=TOKEN_LIMIT_PROXY_CHARS, overlap_words=50)\n",
        "                    final_content_chunks.extend(sub_chunks)\n",
        "                elif str(chunk_item).strip():\n",
        "                    final_content_chunks.append(str(chunk_item))\n",
        "            print(f\"  -> Processed into {len(final_content_chunks)} embedding-safe chunks.\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Gemini provided an entity but no content chunks. Coverage will be 0 if no other chunks are found.\")\n",
        "\n",
        "        content_chunks_for_scoring = final_content_chunks\n",
        "\n",
        "        num_queries_to_run = num_synthetic_queries if num_synthetic_queries is not None else self.query_fanout.num_queries_default\n",
        "        current_date_str = datetime.datetime.now().strftime(\"%B %d, %Y\")\n",
        "\n",
        "        synthetic_queries_pred = self.query_fanout( # CORRECTED CALL\n",
        "            entity_name=str(main_entity),\n",
        "            current_date_str=current_date_str,      # Pass current_date_str\n",
        "            num_queries=num_queries_to_run\n",
        "        )\n",
        "        synthetic_queries = synthetic_queries_pred.synthetic_queries\n",
        "        reasoning_text = synthetic_queries_pred.reasoning_about_facets if hasattr(synthetic_queries_pred, 'reasoning_about_facets') else \"N/A\"\n",
        "\n",
        "        print(f\"ğŸ” Generated {len(synthetic_queries)} synthetic queries for '{main_entity}':\")\n",
        "        for i, q_text in enumerate(synthetic_queries):\n",
        "            print(f\"  {i+1}. {q_text}\")\n",
        "\n",
        "        if not synthetic_queries or not content_chunks_for_scoring:\n",
        "            final_coverage_score = 0.0\n",
        "            audit_details = [{'query': sq, 'covered': False, 'max_similarity': 0.0, 'best_chunk': ''} for sq in synthetic_queries] if synthetic_queries else []\n",
        "            if not synthetic_queries:\n",
        "                print(\"No synthetic queries to assess.\")\n",
        "            if not content_chunks_for_scoring and synthetic_queries:\n",
        "                print(\"No content chunks to assess against generated queries. Coverage will be 0.\")\n",
        "        else:\n",
        "            covered_count = 0\n",
        "            audit_details = []\n",
        "            print(\"\\nğŸ•µï¸ Assessing coverage for each synthetic query...\")\n",
        "\n",
        "            for query_text_item in synthetic_queries:\n",
        "                current_coverage_threshold = coverage_threshold if coverage_threshold is not None else self.coverage_scorer.similarity_threshold_default\n",
        "\n",
        "                coverage_pred = self.coverage_scorer(\n",
        "                    query=query_text_item,\n",
        "                    content_chunks=content_chunks_for_scoring,\n",
        "                    similarity_threshold=current_coverage_threshold\n",
        "                )\n",
        "                status = \"âœ… Covered\" if coverage_pred.is_covered else \"âŒ Not Covered\"\n",
        "                print(f\"  - Query: '{str(query_text_item)[:70]}...' -> {status} (Max Similarity: {coverage_pred.max_similarity:.2f})\")\n",
        "\n",
        "                if coverage_pred.is_covered:\n",
        "                    covered_count += 1\n",
        "                audit_details.append({\n",
        "                    'query': query_text_item,\n",
        "                    'covered': coverage_pred.is_covered,\n",
        "                    'max_similarity': coverage_pred.max_similarity,\n",
        "                    'best_chunk': coverage_pred.best_matching_chunk\n",
        "                })\n",
        "            final_coverage_score = (covered_count / len(synthetic_queries)) * 100 if synthetic_queries else 0.0\n",
        "\n",
        "        print(f\"\\nğŸ“Š Final Coverage Score for '{main_entity}' on {url}: {final_coverage_score:.2f}%\")\n",
        "\n",
        "        return dspy.Prediction(\n",
        "            entity_name=str(main_entity),\n",
        "            url=url,\n",
        "            coverage_score=final_coverage_score,\n",
        "            audit_details=audit_details,\n",
        "            reasoning_about_facets=reasoning_text\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run_audit_markdown",
      "metadata": {
        "id": "run_audit_markdown"
      },
      "source": [
        "### 5. Run the Audit with Interactive Inputs (updated with CoT reasoning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "WZVeIRfpI4sT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WZVeIRfpI4sT",
        "outputId": "71657731-3e1c-4f28-d2c0-6d61eb94ec54",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš€ Starting AI Visibility Audit for: https://francescoragusa.com/content-audit/\n",
            "Processing URL with genai.Client (gemini-2.5-flash-preview-05-20): https://francescoragusa.com/content-audit/\n",
            "âœ… Found 1 grounded content chunks.\n",
            "âœ… Gemini identified entity: 'The webpage is titled \"Content Audit: guida + Template Gratuito - Francesco Ragusa\" and discusses in detail what a content audit is, its purpose in SEO, how to perform one, and how to make decisions about website content. It also mentions content pruning and provides a free template. The entire content revolves around the topic of content auditing'\n",
            "ğŸ§  Main entity identified by Gemini: 'The webpage is titled \"Content Audit: guida + Template Gratuito - Francesco Ragusa\" and discusses in detail what a content audit is, its purpose in SEO, how to perform one, and how to make decisions about website content. It also mentions content pruning and provides a free template. The entire content revolves around the topic of content auditing'\n",
            "ğŸ“„ Obtained 1 grounded content chunk(s) from Gemini. Validating for embedding limits...\n",
            "  -> Processed into 1 embedding-safe chunks.\n",
            "ğŸ§  Reasoning for 'The webpage is titled \"Content Audit: guida + Template Gratuito - Francesco Ragusa\" and discusses in detail what a content audit is, its purpose in SEO, how to perform one, and how to make decisions about website content. It also mentions content pruning and provides a free template. The entire content revolves around the topic of content auditing' (Date: October 31, 2025, Num queries: 20):\n",
            "Key information facets for \"Content Audit\" include:\n",
            "1.  **Definition & Purpose:** Understanding what a content audit is, why it's performed, and its specific benefits for SEO and overall website performance.\n",
            "2.  **Methodology & Process:** The step-by-step guide on how to conduct an audit, including data collection, analysis, and common challenges.\n",
            "3.  **Tools & Resources:** Practical aids like software, templates, and metrics used in the auditing process.\n",
            "4.  **Decision Making & Action:** How to interpret audit results and make informed decisions about content (e.g., update, prune, merge, delete) and integrate it into a broader content strategy.\n",
            "5.  **Current Trends & Best Practices:** Information on the latest approaches, frequency, and strategic considerations for content audits, relevant to October 2025.\n",
            "These facets are crucial for a comprehensive understanding, moving from theoretical knowledge to practical application and strategic integration.\n",
            "ğŸ” Generated 20 synthetic queries for 'The webpage is titled \"Content Audit: guida + Template Gratuito - Francesco Ragusa\" and discusses in detail what a content audit is, its purpose in SEO, how to perform one, and how to make decisions about website content. It also mentions content pruning and provides a free template. The entire content revolves around the topic of content auditing':\n",
            "  1. what is a content audit and its purpose in SEO\n",
            "  2. what are the key steps to perform a content audit\n",
            "  3. how does a content audit improve website SEO performance\n",
            "  4. what data points should be collected for a content audit\n",
            "  5. explain the concept of content pruning in SEO\n",
            "  6. what are the benefits of using a content audit template\n",
            "  7. how to decide whether to update, delete, or merge content after an audit\n",
            "  8. recommended tools for conducting a content audit\n",
            "  9. how often should a website perform a content audit\n",
            "  10. what is the difference between a content audit and a content inventory\n",
            "  11. how to conduct a content audit for a large-scale website\n",
            "  12. best practices for content auditing in 2025\n",
            "  13. what are the common challenges in performing a content audit\n",
            "  14. how does a content audit contribute to content strategy\n",
            "  15. where can I find a free content audit template\n",
            "  16. what metrics are most important when evaluating content in an audit\n",
            "  17. how to present content audit findings to stakeholders\n",
            "  18. examples of successful content audit outcomes\n",
            "  19. what is the role of user experience (UX) in content auditing\n",
            "  20. how to prioritize content for auditing based on business goals\n",
            "\n",
            "ğŸ•µï¸ Assessing coverage for each synthetic query...\n",
            "  - Query: 'what is a content audit and its purpose in SEO...' -> âœ… Covered (Max Similarity: 0.70)\n",
            "  - Query: 'what are the key steps to perform a content audit...' -> âœ… Covered (Max Similarity: 0.73)\n",
            "  - Query: 'how does a content audit improve website SEO performance...' -> âŒ Not Covered (Max Similarity: 0.64)\n",
            "  - Query: 'what data points should be collected for a content audit...' -> âœ… Covered (Max Similarity: 0.68)\n",
            "  - Query: 'explain the concept of content pruning in SEO...' -> âŒ Not Covered (Max Similarity: 0.50)\n",
            "  - Query: 'what are the benefits of using a content audit template...' -> âœ… Covered (Max Similarity: 0.72)\n",
            "  - Query: 'how to decide whether to update, delete, or merge content after an aud...' -> âŒ Not Covered (Max Similarity: 0.59)\n",
            "  - Query: 'recommended tools for conducting a content audit...' -> âœ… Covered (Max Similarity: 0.70)\n",
            "  - Query: 'how often should a website perform a content audit...' -> âŒ Not Covered (Max Similarity: 0.64)\n",
            "  - Query: 'what is the difference between a content audit and a content inventory...' -> âŒ Not Covered (Max Similarity: 0.64)\n",
            "  - Query: 'how to conduct a content audit for a large-scale website...' -> âœ… Covered (Max Similarity: 0.67)\n",
            "  - Query: 'best practices for content auditing in 2025...' -> âŒ Not Covered (Max Similarity: 0.64)\n",
            "  - Query: 'what are the common challenges in performing a content audit...' -> âœ… Covered (Max Similarity: 0.66)\n",
            "  - Query: 'how does a content audit contribute to content strategy...' -> âœ… Covered (Max Similarity: 0.68)\n",
            "  - Query: 'where can I find a free content audit template...' -> âœ… Covered (Max Similarity: 0.78)\n",
            "  - Query: 'what metrics are most important when evaluating content in an audit...' -> âŒ Not Covered (Max Similarity: 0.64)\n",
            "  - Query: 'how to present content audit findings to stakeholders...' -> âŒ Not Covered (Max Similarity: 0.61)\n",
            "  - Query: 'examples of successful content audit outcomes...' -> âŒ Not Covered (Max Similarity: 0.61)\n",
            "  - Query: 'what is the role of user experience (UX) in content auditing...' -> âŒ Not Covered (Max Similarity: 0.64)\n",
            "  - Query: 'how to prioritize content for auditing based on business goals...' -> âŒ Not Covered (Max Similarity: 0.61)\n",
            "\n",
            "ğŸ“Š Final Coverage Score for 'The webpage is titled \"Content Audit: guida + Template Gratuito - Francesco Ragusa\" and discusses in detail what a content audit is, its purpose in SEO, how to perform one, and how to make decisions about website content. It also mentions content pruning and provides a free template. The entire content revolves around the topic of content auditing' on https://francescoragusa.com/content-audit/: 45.00%\n",
            "\n",
            "ğŸ’¾ Audit results saved to ai_visibility_audit_the_webpage_is_titled_content_.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-852474179.py:40: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  'audit_timestamp_utc': datetime.datetime.utcnow().isoformat(),\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1>AI Visibility Audit: The webpage is titled \"Content Audit: guida + Template Gratuito - Francesco Ragusa\" and discusses in detail what a content audit is, its purpose in SEO, how to perform one, and how to make decisions about website content. It also mentions content pruning and provides a free template. The entire content revolves around the topic of content auditing</h1><p><b>URL:</b> <a href='https://francescoragusa.com/content-audit/' target='_blank'>https://francescoragusa.com/content-audit/</a></p><p><b>Overall AI Query Fan-Out Coverage:</b> 45.00%</p><h3>Synthetic Query Coverage Details:</h3><ul><li>âœ… <b>Query:</b> what is a content audit and its purpose in SEO (Max Similarity: 0.70)</li><li>âœ… <b>Query:</b> what are the key steps to perform a content audit (Max Similarity: 0.73)</li><li>âŒ <b>Query:</b> how does a content audit improve website SEO performance (Max Similarity: 0.64)</li><li>âœ… <b>Query:</b> what data points should be collected for a content audit (Max Similarity: 0.68)</li><li>âŒ <b>Query:</b> explain the concept of content pruning in SEO (Max Similarity: 0.50)</li><li>âœ… <b>Query:</b> what are the benefits of using a content audit template (Max Similarity: 0.72)</li><li>âŒ <b>Query:</b> how to decide whether to update, delete, or merge content after an audit (Max Similarity: 0.59)</li><li>âœ… <b>Query:</b> recommended tools for conducting a content audit (Max Similarity: 0.70)</li><li>âŒ <b>Query:</b> how often should a website perform a content audit (Max Similarity: 0.64)</li><li>âŒ <b>Query:</b> what is the difference between a content audit and a content inventory (Max Similarity: 0.64)</li><li>âœ… <b>Query:</b> how to conduct a content audit for a large-scale website (Max Similarity: 0.67)</li><li>âŒ <b>Query:</b> best practices for content auditing in 2025 (Max Similarity: 0.64)</li><li>âœ… <b>Query:</b> what are the common challenges in performing a content audit (Max Similarity: 0.66)</li><li>âœ… <b>Query:</b> how does a content audit contribute to content strategy (Max Similarity: 0.68)</li><li>âœ… <b>Query:</b> where can I find a free content audit template (Max Similarity: 0.78)</li><li>âŒ <b>Query:</b> what metrics are most important when evaluating content in an audit (Max Similarity: 0.64)</li><li>âŒ <b>Query:</b> how to present content audit findings to stakeholders (Max Similarity: 0.61)</li><li>âŒ <b>Query:</b> examples of successful content audit outcomes (Max Similarity: 0.61)</li><li>âŒ <b>Query:</b> what is the role of user experience (UX) in content auditing (Max Similarity: 0.64)</li><li>âŒ <b>Query:</b> how to prioritize content for auditing based on business goals (Max Similarity: 0.61)</li></ul>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4e4a55ae-8553-411a-9d59-7dc8ee738805\", \"ai_visibility_audit_the_webpage_is_titled_content_.json\", 5023)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Cell 5: Run the Audit with Interactive Inputs\n",
        "\n",
        "#@title Run AI Visibility Audit\n",
        "#@markdown Enter the URL to analyze and adjust the parameters for the simulation.\n",
        "url_to_audit_form = 'https://francescoragusa.com/content-audit/' #@param {type:\"string\"}\n",
        "num_synthetic_queries_form = 20 #@param {type:\"integer\", min:3, max:10, step:1}\n",
        "coverage_threshold_form = 0.65 #@param {type:\"number\", min:0.5, max:0.9, step:0.01}\n",
        "\n",
        "# --- Pre-run Check ---\n",
        "# Check if the necessary clients and settings are configured from Cell 2\n",
        "can_run_audit = True\n",
        "if not 'GEMINI_API_KEY' in globals() or not GEMINI_API_KEY:\n",
        "    print(\"âŒ ERROR: GEMINI_API_KEY is not configured. Please set it in Cell 2 and re-run.\")\n",
        "    can_run_audit = False\n",
        "if not 'dspy' in globals() or not hasattr(dspy.settings, 'lm') or not dspy.settings.lm:\n",
        "    print(\"âŒ ERROR: DSPy Language Model (lm) is not configured. Please run Cell 2.\")\n",
        "    can_run_audit = False\n",
        "if not 'gemini_sdk_client_for_utils' in globals() or not gemini_sdk_client_for_utils:\n",
        "    print(\"âŒ ERROR: Shared Gemini Client (for utilities) is not initialized. Please run Cell 2.\")\n",
        "    can_run_audit = False\n",
        "\n",
        "if can_run_audit:\n",
        "    # --- Execute Audit ---\n",
        "    # Instantiate the auditor with default values from the form.\n",
        "    # The forward method will use these unless overridden.\n",
        "    auditor = AIVisibilityAuditor(\n",
        "        num_synthetic_queries_default=num_synthetic_queries_form,\n",
        "        coverage_threshold_default=coverage_threshold_form\n",
        "    )\n",
        "\n",
        "    audit_result = auditor(url=url_to_audit_form)\n",
        "\n",
        "    # --- Process and Display Results ---\n",
        "    if audit_result:\n",
        "        # Prepare JSON output for a detailed report\n",
        "        output_data = {\n",
        "            'url': audit_result.url,\n",
        "            'entity_name': audit_result.entity_name,\n",
        "            'overall_coverage_score_percent': audit_result.coverage_score,\n",
        "            'audit_timestamp_utc': datetime.datetime.utcnow().isoformat(),\n",
        "            'parameters': {\n",
        "                'num_synthetic_queries_requested': num_synthetic_queries_form,\n",
        "                'num_synthetic_queries_generated': len(audit_result.audit_details) if audit_result.audit_details else 0,\n",
        "                'coverage_threshold_used': coverage_threshold_form\n",
        "            },\n",
        "            'llm_reasoning_for_queries': audit_result.reasoning_steps if hasattr(audit_result, 'reasoning_steps') else \"N/A\",\n",
        "            'synthetic_query_details': audit_result.audit_details,\n",
        "            'llm_provider': JSON_REPORT_LLM_PROVIDER, # Variable from Cell 2\n",
        "            'llm_model_used': JSON_REPORT_LLM_MODEL, # Variable from Cell 2\n",
        "            'embedding_provider': JSON_REPORT_EMBEDDING_PROVIDER, # Variable from Cell 2\n",
        "            'embedding_model_used': JSON_REPORT_EMBEDDING_MODEL # Variable from Cell 2\n",
        "        }\n",
        "\n",
        "        # Create a safe filename for the JSON report\n",
        "        entity_file_name_part = \"audit_report\" # Default\n",
        "        if audit_result.entity_name and isinstance(audit_result.entity_name, str) and \"Error\" not in audit_result.entity_name:\n",
        "             entity_file_name_part = re.sub(r'[^a-zA-Z0-9_]', '', audit_result.entity_name.replace(' ', '_')).lower()[:30]\n",
        "        output_filename = f\"ai_visibility_audit_{entity_file_name_part}.json\"\n",
        "\n",
        "        with open(output_filename, 'w') as f:\n",
        "            json.dump(output_data, f, indent=2)\n",
        "        print(f\"\\nğŸ’¾ Audit results saved to {output_filename}\")\n",
        "\n",
        "        # Display a user-friendly HTML summary in the notebook\n",
        "        html_summary = f\"<h1>AI Visibility Audit: {audit_result.entity_name}</h1>\"\n",
        "        html_summary += f\"<p><b>URL:</b> <a href='{audit_result.url}' target='_blank'>{audit_result.url}</a></p>\"\n",
        "        html_summary += f\"<p><b>Overall AI Query Fan-Out Coverage:</b> {audit_result.coverage_score:.2f}%</p>\"\n",
        "\n",
        "        if hasattr(audit_result, 'reasoning_steps') and audit_result.reasoning_steps and audit_result.reasoning_steps != \"N/A\":\n",
        "            escaped_reasoning = audit_result.reasoning_steps.replace('&', '&').replace('<', '<').replace('>', '>')\n",
        "            html_summary += f\"<h3>LLM Reasoning for Query Generation:</h3><pre style='white-space: pre-wrap; background-color: #f0f0f0; padding: 10px; border-radius: 5px; font-family: monospace;'>{escaped_reasoning}</pre>\"\n",
        "\n",
        "        html_summary += \"<h3>Synthetic Query Coverage Details:</h3><ul>\"\n",
        "        if audit_result.audit_details:\n",
        "            for detail in audit_result.audit_details:\n",
        "                status_emoji = \"âœ…\" if detail['covered'] else \"âŒ\"\n",
        "                escaped_query = str(detail['query']).replace('&', '&').replace('<', '<').replace('>', '>')\n",
        "                html_summary += f\"<li>{status_emoji} <b>Query:</b> {escaped_query} (Max Similarity: {detail['max_similarity']:.2f})</li>\"\n",
        "        else:\n",
        "            html_summary += \"<li>No synthetic queries were processed or available.</li>\"\n",
        "        html_summary += \"</ul>\"\n",
        "\n",
        "        display(HTML(html_summary))\n",
        "\n",
        "        # Offer the JSON report for download in Colab\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(output_filename)\n",
        "        except ImportError:\n",
        "            print(f\"To download the report outside Colab, find '{output_filename}' in your current directory.\")\n",
        "    else:\n",
        "        print(\"Audit did not produce a result. Please check the log for errors in previous steps.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "install_markdown",
        "setup_markdown",
        "utilities_markdown"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}